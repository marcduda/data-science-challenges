{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data with the features.\n",
    "The airfare in euro is the target value.\n",
    "We hot encode the categorical feature cabin_class and drop one.\n",
    "To tune the regressor, we divide the data into training, testing and validation sets.\n",
    "I ended up not using the distance_code feature since when I tried to encode it, it took too much space and I couldn't continu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict   \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "df = pd.read_csv('features_regression.csv')\n",
    "df = df.drop(['search_id'], axis=1) \n",
    "#df = df.values\n",
    "y = df['fare_eur'].values\n",
    "X = df.drop('fare_eur',1)\n",
    "dummies = pd.get_dummies(X[['cabin_class']])\n",
    "X = pd.concat([X, dummies], axis=1)\n",
    "X = X.drop(['cabin_class','cabin_class_mixed','distance_code'], 1)\n",
    "#%%\n",
    "X_, X_test, y_, y_test = train_test_split(X,y, test_size=0.40, random_state=42)#[:20000] \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_ ,y_ , test_size=0.40, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use a random forest regressor, because it is a powerful algorithm that is easy to tune.\n",
    "There are only two parameters, the number of features considered at each level of the tree (for regression, in auto mode considering all the features is recommended) and the number of tree to build.\n",
    "Since the results improve with a growing number of trees, I only looked at a model for which the calculation time is acceptable. \n",
    "First I wanted to use a cross validation algorithm to see the performance of the model but due to the processing time, I didn't and simply looked at some metrics on the testing set.\n",
    "I also plotted a figure to see the importance of each features and it is the partner_id that has the most influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = [10,50,100,150]\n",
    "metric = []\n",
    "for n_estimator in n_estimators:\n",
    "    forest = RandomForestRegressor(n_estimator, max_features='auto')\n",
    "    forest.fit(X_train,y_train)\n",
    "    scores = forest.predict(X_test)#cross_val_predict(forest, X_train, y_train, cv=10)\n",
    "    metric.append(metrics.mean_squared_error(y_test, scores))\n",
    "    print(metrics.mean_squared_error(y_test, scores))\n",
    "    \n",
    "    forest.fit(X_train, y_train)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), indices)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose 100 trees as the best number of trees to build the model because the processing time was acceptable.\n",
    "To be sure that I don't tune in too much the model, I predicted some data that the model has never see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_n_estimator = 100\n",
    "forest_model = RandomForestRegressor(n_estimator, max_features='auto')\n",
    "forest_model.fit(X_train,y_train)\n",
    "        \n",
    "predicted_valid = forest_model.predict(X_valid)\n",
    "print(\"Regression problem, metrics on validation set:\")\n",
    "print(metrics.classification_report(y_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "I was not satisfied with my feature that accounted for the distance so I would work more on that.\n",
    "The fact that the utc offset was not taken into account in the datetimes is also a thing to improve.\n",
    "Overall, I would work more on the features, try to find more meaningful features for the airfare prediction businesswise and look at the columns that I didn't consider.\n",
    "I might also look at the influence of features on the model to go back and forth when engineering the features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
